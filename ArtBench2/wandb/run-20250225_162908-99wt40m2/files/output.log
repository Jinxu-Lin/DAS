02/25/2025 16:29:10 - INFO - __main__ - ***** Running training *****
02/25/2025 16:29:10 - INFO - __main__ -   Num Epochs = 100
02/25/2025 16:29:10 - INFO - __main__ -   Instantaneous batch size per device = 128
02/25/2025 16:29:10 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 256
02/25/2025 16:29:10 - INFO - __main__ -   Gradient Accumulation steps = 1
02/25/2025 16:29:10 - INFO - __main__ -   Total optimization steps = 2000
Steps:   0%|                                                                                                                                                                                           | 0/2000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "1_train.py", line 378, in <module>
    main(args)
  File "1_train.py", line 295, in main
    for step, batch in enumerate(train_dataloader):
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/accelerate/data_loader.py", line 543, in __iter__
    synchronize_rng_states(self.rng_types, self.synchronized_generator)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/accelerate/utils/random.py", line 132, in synchronize_rng_states
    synchronize_rng_state(RNGType(rng_type), generator=generator)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/accelerate/utils/random.py", line 106, in synchronize_rng_state
    torch.distributed.broadcast(rng_state, 0)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
    work = default_pg.broadcast([tensor], opts)
torch.distributed.DistBackendError: NCCL error in: /opt/conda/conda-bld/pytorch_1699449229234/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.18.6
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 1000
