02/25/2025 17:14:46 - INFO - __main__ - ***** Running training *****
02/25/2025 17:14:46 - INFO - __main__ -   Num examples = 1221
02/25/2025 17:14:46 - INFO - __main__ -   Num Epochs = 50
02/25/2025 17:14:46 - INFO - __main__ -   Instantaneous batch size per device = 1
02/25/2025 17:14:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
02/25/2025 17:14:46 - INFO - __main__ -   Gradient Accumulation steps = 4
02/25/2025 17:14:46 - INFO - __main__ -   Total optimization steps = 15000
Steps:   0%|                                                                                                                                                              | 0/15000 [00:01<?, ?it/s, lr=0.0001, step_loss=0.223]Traceback (most recent call last):
  File "train_test.py", line 975, in <module>
    main()
  File "train_test.py", line 856, in main
    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/accelerate/accelerator.py", line 2396, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/accelerate/accelerator.py", line 2340, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 307, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
  File "/home/jinxulin/anaconda3/envs/miss/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 229, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
